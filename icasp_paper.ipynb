{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "# Config (feel free to tweak)\n",
    "# ================================================================\n",
    "NUM_USERS = 20\n",
    "K = 4               # <- generic subset size\n",
    "ROUNDS = 2000               # make 2k if you want; 1k is faster to iterate\n",
    "LOCAL_EPOCHS = 5            # local steps per selection\n",
    "DIM = 100\n",
    "SAMPLES_PER_USER = 3000 // NUM_USERS\n",
    "LR = 0.01\n",
    "SEEDS = [0, 1, 2, 3, 4]     # five seeds\n",
    "NUM_SAMPLES_FOR_Q = 1_000_000   # Monte Carlo to estimate q over K-subsets\n",
    "\n",
    "IPFP_TOL = 1e-12\n",
    "IPFP_MAX_ITERS = 10000\n",
    "\n",
    "# ================================================================\n",
    "# Utilities: masked IPFP (unchanged in spirit, generalized for any K)\n",
    "# ================================================================\n",
    "def build_mask(n, subsets):\n",
    "    \"\"\"\n",
    "    Boolean mask M (n x m) with 1-based subset entries:\n",
    "    M[i, j] = True iff (i+1) is in subset j; rows are 0..n-1, subsets are tuples in {1,..,n}.\n",
    "    \"\"\"\n",
    "    m = len(subsets)\n",
    "    M = np.zeros((n, m), dtype=bool)\n",
    "    for j, Aj in enumerate(subsets):\n",
    "        for i in Aj:       # i is 1..n\n",
    "            M[i-1, j] = True\n",
    "    return M\n",
    "\n",
    "def initialize_Y(p, q, M, mode=\"uniform\"):\n",
    "    n, m = M.shape\n",
    "    Y = np.zeros((n, m), dtype=float)\n",
    "    for j in range(m):\n",
    "        rows = np.where(M[:, j])[0]\n",
    "        if len(rows) == 0:\n",
    "            if q[j] > 0:\n",
    "                raise ValueError(f\"Column j={j} has empty subset but q[j]={q[j]}>0.\")\n",
    "            else:\n",
    "                continue\n",
    "        if q[j] == 0.0:\n",
    "            continue\n",
    "        if mode == \"uniform\":\n",
    "            Y[rows, j] = q[j] / len(rows)\n",
    "        elif mode == \"p_proportional\":\n",
    "            pj = p[rows]\n",
    "            s = pj.sum()\n",
    "            if s > 0:\n",
    "                Y[rows, j] = q[j] * (pj / s)\n",
    "            else:\n",
    "                Y[rows, j] = q[j] / len(rows)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown init mode\")\n",
    "    return Y\n",
    "\n",
    "def ipfp_masked(p, q, M, tol=1e-10, max_iter=10000, verbose=False):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    n, m = M.shape\n",
    "\n",
    "    if p.ndim != 1 or q.ndim != 1:\n",
    "        raise ValueError(\"p and q must be 1D arrays\")\n",
    "    if not np.all(p >= 0) or not np.all(q >= 0):\n",
    "        raise ValueError(\"p and q must be nonnegative\")\n",
    "    if abs(p.sum() - q.sum()) > 1e-12:\n",
    "        raise ValueError(f\"Totals must match: sum(p)={p.sum()} vs sum(q)={q.sum()}\")\n",
    "\n",
    "    Y = initialize_Y(p, q, M, mode=\"uniform\")\n",
    "    rows_idx = [np.where(M[i, :])[0] for i in range(n)]\n",
    "    cols_idx = [np.where(M[:, j])[0] for j in range(m)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Row scaling\n",
    "        row_sums = Y.sum(axis=1)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            row_scale = np.ones(n)\n",
    "            need = (row_sums > 0)\n",
    "            row_scale[need] = p[need] / row_sums[need]\n",
    "        infeasible_rows = (row_sums == 0) & (p > 0)\n",
    "        if np.any(infeasible_rows):\n",
    "            i = np.where(infeasible_rows)[0][0]\n",
    "            raise ValueError(f\"Infeasible: row i={i} has no support but p[i]={p[i]}>0\")\n",
    "        for i in range(n):\n",
    "            js = rows_idx[i]\n",
    "            if js.size:\n",
    "                Y[i, js] *= row_scale[i]\n",
    "\n",
    "        # Column scaling\n",
    "        col_sums = Y.sum(axis=0)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            col_scale = np.ones(m)\n",
    "            needc = (col_sums > 0)\n",
    "            col_scale[needc] = q[needc] / col_sums[needc]\n",
    "        infeasible_cols = (col_sums == 0) & (q > 0)\n",
    "        if np.any(infeasible_cols):\n",
    "            j = np.where(infeasible_cols)[0][0]\n",
    "            raise ValueError(f\"Infeasible: column j={j} has no support but q[j]={q[j]}>0\")\n",
    "        for j in range(m):\n",
    "            is_ = cols_idx[j]\n",
    "            if is_.size:\n",
    "                Y[is_, j] *= col_scale[j]\n",
    "\n",
    "        row_err = np.linalg.norm(Y.sum(axis=1) - p, ord=1)\n",
    "        col_err = np.linalg.norm(Y.sum(axis=0) - q, ord=1)\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"[{it}] row_err={row_err:.3e}, col_err={col_err:.3e}\")\n",
    "        if max(row_err, col_err) <= tol:\n",
    "            return Y, {\"iters\": it+1, \"row_err\": row_err, \"col_err\": col_err}\n",
    "\n",
    "    return Y, {\"iters\": max_iter, \"row_err\": row_err, \"col_err\": col_err, \"warning\": \"max_iter reached\"}\n",
    "\n",
    "def recover_T_from_Y(Y, q, M, p, strategy_for_q0=\"p_restricted\"):\n",
    "    n, m = Y.shape\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    T = np.zeros_like(Y)\n",
    "\n",
    "    pos = q > 0\n",
    "    T[:, pos] = Y[:, pos] / q[pos]\n",
    "\n",
    "    zero_cols = np.where(~pos)[0]\n",
    "    for j in zero_cols:\n",
    "        rows = np.where(M[:, j])[0]\n",
    "        if rows.size == 0:\n",
    "            raise ValueError(f\"Column j={j} has empty subset; cannot define T.\")\n",
    "        if strategy_for_q0 == \"p_restricted\":\n",
    "            w = p[rows]\n",
    "            s = w.sum()\n",
    "            if s > 0:\n",
    "                T[rows, j] = w / s\n",
    "            else:\n",
    "                T[rows, j] = 1.0 / rows.size\n",
    "        elif strategy_for_q0 == \"uniform\":\n",
    "            T[rows, j] = 1.0 / rows.size\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy_for_q0\")\n",
    "\n",
    "    T[~M] = 0.0\n",
    "    col_sums = T.sum(axis=0)\n",
    "    for j in range(m):\n",
    "        if col_sums[j] != 0:\n",
    "            T[:, j] /= col_sums[j]\n",
    "    return T\n",
    "\n",
    "def solve_T_with_given_subsets(p, q, subsets, tol=1e-10, max_iter=10000, verbose=False):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    n = p.size\n",
    "    m = len(subsets)\n",
    "    if q.size != m:\n",
    "        raise ValueError(f\"Length of q ({q.size}) must equal number of subsets ({m}).\")\n",
    "\n",
    "    M = build_mask(n, subsets)  # 1-based subsets\n",
    "    if abs(p.sum() - q.sum()) > 1e-12:\n",
    "        p = p / p.sum()\n",
    "        q = q / q.sum()\n",
    "\n",
    "    Y, info = ipfp_masked(p, q, M, tol=tol, max_iter=max_iter, verbose=verbose)\n",
    "    T = recover_T_from_Y(Y, q, M, p, strategy_for_q0=\"p_restricted\")\n",
    "\n",
    "    col_err = np.max(np.abs(T.sum(axis=0) - 1))\n",
    "    recon_p = T @ q\n",
    "    p_err = np.max(np.abs(recon_p - p))\n",
    "    info.update({\"T_col_err_inf\": float(col_err), \"p_match_err_inf\": float(p_err)})\n",
    "    return T, subsets, M, info\n",
    "\n",
    "def column_users_and_weights(T, subsets, j):\n",
    "    \"\"\"\n",
    "    Return (0-based user ids, normalized weights) for column j (subset of size K).\n",
    "    \"\"\"\n",
    "    Aj_1 = subsets[j]                      # 1-based subset tuple\n",
    "    rows = np.array([i-1 for i in Aj_1])   # to 0-based\n",
    "    w = T[rows, j]\n",
    "    s = w.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        w = np.full_like(w, 1.0 / len(w))\n",
    "    else:\n",
    "        w = w / s\n",
    "    return rows, w\n",
    "\n",
    "# ================================================================\n",
    "# Linear regression helpers\n",
    "# ================================================================\n",
    "def local_train_lr(X, y, w_init, epochs=1, lr=0.01):\n",
    "    w = w_init.copy()\n",
    "    n = len(y)\n",
    "    for _ in range(epochs):\n",
    "        preds = X @ w\n",
    "        grad = (X.T @ (preds - y)) / n\n",
    "        w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def global_weighted_mse(w, user_data, p):\n",
    "    tot = 0.0\n",
    "    for i, (Xi, yi) in enumerate(user_data):\n",
    "        preds = Xi @ w\n",
    "        loss = mean_squared_error(yi, preds)\n",
    "        tot += p[i] * loss\n",
    "    return tot\n",
    "\n",
    "# ================================================================\n",
    "# Build skewed p, r and K-subsets; estimate q via Monte Carlo\n",
    "# ================================================================\n",
    "def make_skew_distributions(N):\n",
    "    idx_1 = np.arange(1, N + 1, dtype=float)\n",
    "    # r_i ∝ exp(+i) (prior for availability)\n",
    "    r = np.power(idx_1, 3)\n",
    "    r /= r.sum()\n",
    "    # p_i ∝ exp(-i) (importance for objective)\n",
    "    p = np.power(idx_1[::-1], 3)\n",
    "    p /= p.sum()\n",
    "    return p, r\n",
    "\n",
    "def all_K_subsets_1based(N, K):\n",
    "    return [tuple(c) for c in combinations(range(1, N + 1), K)]\n",
    "\n",
    "def estimate_q_by_mc(subsets, r, N, K, num_samples=1_000_000, rng=None):\n",
    "    \"\"\"\n",
    "    Estimate q over the provided 'subsets' (1-based tuples of size K)\n",
    "    by Monte Carlo: S ~ Choice(N, K, p=r, replace=False).\n",
    "    Efficiently tallies unique rows using np.unique.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(0)\n",
    "    # map subset tuple -> index\n",
    "    subset_to_idx = {s: j for j, s in enumerate(subsets)}\n",
    "    # sample many subsets\n",
    "    draws = rng.choice(N, size=(num_samples, K), replace=True, p=r)\n",
    "    draws.sort(axis=1)  # canonical order\n",
    "    # convert to 1-based to match 'subsets'\n",
    "    draws_1b = draws + 1\n",
    "    # unique counts\n",
    "    uniq, counts = np.unique(draws_1b, axis=0, return_counts=True)\n",
    "    q_counts = np.zeros(len(subsets), dtype=np.int64)\n",
    "    # assign counts to indices\n",
    "    for row, c in zip(uniq, counts):\n",
    "        tup = tuple(row.tolist())\n",
    "        j = subset_to_idx.get(tup, None)\n",
    "        if j is not None:\n",
    "            q_counts[j] += c\n",
    "    q = q_counts.astype(float)\n",
    "    q /= q.sum()\n",
    "    return q\n",
    "\n",
    "# ================================================================\n",
    "# Main experiment: FedOT(K) vs FedAvg(K, EXACT) vs FedAvg(full)\n",
    "# ================================================================\n",
    "\n",
    "all_losses_fedot = []\n",
    "all_losses_faK   = []\n",
    "all_losses_full  = []\n",
    "\n",
    "# Generate data once (we'll use the same data for all seeds for fair comparison)\n",
    "X_full, y_full = make_regression(n_samples=3000, n_features=DIM, noise=0.1, random_state=42)\n",
    "X_full = StandardScaler().fit_transform(X_full)\n",
    "\n",
    "# Partition to users\n",
    "perm = np.random.permutation(len(X_full))\n",
    "X = X_full[perm]\n",
    "y = y_full[perm]\n",
    "user_data = [\n",
    "    (X[i * SAMPLES_PER_USER:(i + 1) * SAMPLES_PER_USER],\n",
    "        y[i * SAMPLES_PER_USER:(i + 1) * SAMPLES_PER_USER])\n",
    "    for i in range(NUM_USERS)\n",
    "]\n",
    "\n",
    "# Distributions and subset family\n",
    "p, r = make_skew_distributions(NUM_USERS)\n",
    "subsets_K = all_K_subsets_1based(NUM_USERS, K)\n",
    "print(f\"Total subsets of size K: C({NUM_USERS},{K}) = {len(subsets_K)}\")\n",
    "\n",
    "# Estimate q via Monte Carlo sampling from r\n",
    "rng_init = np.random.RandomState(0)\n",
    "q = estimate_q_by_mc(subsets_K, r, NUM_USERS, K, num_samples=NUM_SAMPLES_FOR_Q, rng=rng_init)\n",
    "\n",
    "# Solve T via masked IPFP for this subset family and q\n",
    "T, subsets_used, M_mask, info = solve_T_with_given_subsets(\n",
    "    p, q, subsets_K, tol=IPFP_TOL, max_iter=IPFP_MAX_ITERS, verbose=False\n",
    ")\n",
    "print(f\"IPFP iters={info.get('iters')} | T col err∞={info.get('T_col_err_inf'):.2e} | p match err∞={info.get('p_match_err_inf'):.2e}\")\n",
    "\n",
    "# --- Estimate q via Monte Carlo sampling from r ---\n",
    "rng_init = np.random.RandomState(0)\n",
    "q = estimate_q_by_mc(subsets_K, r, NUM_USERS, K, num_samples=NUM_SAMPLES_FOR_Q, rng=rng_init)\n",
    "\n",
    "# --- Solve T via masked IPFP for this subset family and q ---\n",
    "T, subsets_used, M_mask, info = solve_T_with_given_subsets(\n",
    "    p, q, subsets_K, tol=IPFP_TOL, max_iter=IPFP_MAX_ITERS, verbose=False\n",
    ")\n",
    "print(f\"  IPFP iters={info.get('iters')} | T col err∞={info.get('T_col_err_inf'):.2e} | p match err∞={info.get('p_match_err_inf'):.2e}\")\n",
    "\n",
    "\n",
    "\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    print(f\"\\n[Seed {seed}]  (K={K})\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    np.random.seed(seed)\n",
    "    # --- Initialize globals for the three methods ---\n",
    "    d = user_data[0][0].shape[1]\n",
    "    w_fedot = np.zeros(d)\n",
    "    w_faK   = np.zeros(d)\n",
    "    w_full  = np.zeros(d)\n",
    "\n",
    "    losses_ot   = []\n",
    "    losses_faK  = []\n",
    "    losses_full = []\n",
    "\n",
    "    q_cum = np.cumsum(q)\n",
    "\n",
    "    # --- Precompute local data references for speed ---\n",
    "    Xs = [ud[0] for ud in user_data]\n",
    "    ys = [ud[1] for ud in user_data]\n",
    "\n",
    "    # --- Training loop ---\n",
    "    for r_idx in range(ROUNDS):\n",
    "        # sample a subset j ~ q\n",
    "        u = rng.random()\n",
    "        j = int(np.searchsorted(q_cum, u, side=\"right\"))\n",
    "\n",
    "        # members of subset j for this round\n",
    "        users_S, weights_T = column_users_and_weights(T, subsets_used, j)  # length K\n",
    "\n",
    "        # ------------------ FedOT(K) ------------------\n",
    "        local_models_ot = []\n",
    "        for uid in users_S:\n",
    "            theta_i = local_train_lr(Xs[uid], ys[uid], w_fedot, epochs=LOCAL_EPOCHS, lr=LR)\n",
    "            local_models_ot.append(theta_i)\n",
    "        # convex combination by T column weights\n",
    "        w_next_ot = np.zeros_like(w_fedot)\n",
    "        for coeff, theta in zip(weights_T, local_models_ot):\n",
    "            w_next_ot += coeff * theta\n",
    "        w_fedot = w_next_ot\n",
    "        losses_ot.append(global_weighted_mse(w_fedot, user_data, p))\n",
    "\n",
    "        # ---------------- FedAvg(K), EXACT: sum_{i in S} (N/K)*p_i * theta_i ----------------\n",
    "        local_models_faK = []\n",
    "        for uid in users_S:\n",
    "            theta_i = local_train_lr(Xs[uid], ys[uid], w_faK, epochs=LOCAL_EPOCHS, lr=LR)\n",
    "            local_models_faK.append((uid, theta_i))\n",
    "        w_next_faK = np.zeros_like(w_faK)\n",
    "        scale = NUM_USERS / float(K)\n",
    "        # pees = np.asarray(list(p[uid] for (uid, theta_i) in local_models_faK)).sum()\n",
    "        # scale = pees\n",
    "        for uid, theta_i in local_models_faK:\n",
    "            w_next_faK += (p[uid] * scale) * theta_i\n",
    "            # w_next_faK += (p[uid] / scale) * theta_i\n",
    "        w_faK = w_next_faK\n",
    "        losses_faK.append(global_weighted_mse(w_faK, user_data, p))\n",
    "\n",
    "        # ---------------- FedAvg (full): sum_i p_i * theta_i ----------------\n",
    "        local_models_full = []\n",
    "        for uid in range(NUM_USERS):\n",
    "            theta_i = local_train_lr(Xs[uid], ys[uid], w_full, epochs=LOCAL_EPOCHS, lr=LR)\n",
    "            local_models_full.append((uid, theta_i))\n",
    "        w_next_full = np.zeros_like(w_full)\n",
    "        for uid, theta_i in local_models_full:\n",
    "            w_next_full += p[uid] * theta_i\n",
    "        w_full = w_next_full\n",
    "        losses_full.append(global_weighted_mse(w_full, user_data, p))\n",
    "\n",
    "        if (r_idx + 1) % 200 == 0:\n",
    "            print(f\"  Round {r_idx+1}/{ROUNDS}: FedOT={losses_ot[-1]:.6f} | \"\n",
    "                  f\"FedAvg(K)={losses_faK[-1]:.6f} | FedAvg(full)={losses_full[-1]:.6f}\")\n",
    "\n",
    "    all_losses_fedot.append(np.array(losses_ot))\n",
    "    all_losses_faK.append(np.array(losses_faK))\n",
    "    all_losses_full.append(np.array(losses_full))\n",
    "\n",
    "# ================================================================\n",
    "# Aggregate across seeds\n",
    "# ================================================================\n",
    "all_losses_fedot = np.vstack(all_losses_fedot)\n",
    "all_losses_faK   = np.vstack(all_losses_faK)\n",
    "all_losses_full  = np.vstack(all_losses_full)\n",
    "\n",
    "loss_fedot_mean = all_losses_fedot.mean(axis=0)\n",
    "loss_fedot_std  = all_losses_fedot.std(axis=0)\n",
    "\n",
    "loss_faK_mean = all_losses_faK.mean(axis=0)\n",
    "loss_faK_std  = all_losses_faK.std(axis=0)\n",
    "\n",
    "loss_full_mean = all_losses_full.mean(axis=0)\n",
    "loss_full_std  = all_losses_full.std(axis=0)\n",
    "\n",
    "# For the bar plots (use last-seed p and r — they are deterministic anyway)\n",
    "p, r = make_skew_distributions(NUM_USERS)\n",
    "\n",
    "# ================================================================\n",
    "# Plot: loss curves + bars for p and r\n",
    "# Colors: FedOT=blue, FedAvg(K)=orange, FedAvg(full)=red\n",
    "# ================================================================\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[2.2, 1.0])\n",
    "\n",
    "# --- (1) Loss curves (span top row) ---\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "x = np.arange(ROUNDS)\n",
    "\n",
    "ax0.plot(x, loss_fedot_mean, label=f\"FedOT (K={K})\", color=\"tab:blue\", linewidth=1.8)\n",
    "ax0.fill_between(x, loss_fedot_mean - loss_fedot_std, loss_fedot_mean + loss_fedot_std,\n",
    "                 alpha=0.15, color=\"tab:blue\")\n",
    "\n",
    "ax0.plot(x, loss_faK_mean, label=f\"FedAvg (K={K})\", color=\"tab:orange\", linewidth=1.8)\n",
    "ax0.fill_between(x, loss_faK_mean - loss_faK_std, loss_faK_mean + loss_faK_std,\n",
    "                 alpha=0.15, color=\"tab:orange\")\n",
    "\n",
    "ax0.plot(x, loss_full_mean, label=\"FedAvg (full devices)\", color=\"tab:red\", linewidth=1.8)\n",
    "ax0.fill_between(x, loss_full_mean - loss_full_std, loss_full_mean + loss_full_std,\n",
    "                 alpha=0.15, color=\"tab:red\")\n",
    "\n",
    "ax0.set_yscale(\"log\")  # optional\n",
    "ax0.set_xlabel(\"Communication round\")\n",
    "ax0.set_ylabel(r\"Global loss $\\sum_i p_i \\,\\mathrm{MSE}_i$ - log\")\n",
    "ax0.set_title(f\"Linear Regression — FedOT vs FedAvg (K={K}) vs FedAvg(full) — mean ± std over {len(SEEDS)} seeds\")\n",
    "ax0.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\")\n",
    "ax0.legend()\n",
    "\n",
    "# --- (2) Bar plot for p ---\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "ax1.bar(np.arange(1, NUM_USERS+1), p, color=\"gray\", edgecolor=\"black\", linewidth=0.6)\n",
    "ax1.set_title(\"Posterior importance distribution $p$ \")\n",
    "ax1.set_xlabel(\"User index\")\n",
    "ax1.set_ylabel(\"p_i\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# --- (3) Bar plot for r ---\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "ax2.bar(np.arange(1, NUM_USERS+1), r, color=\"gray\", edgecolor=\"black\", linewidth=0.6)\n",
    "ax2.set_title(\"Prior availability distribution\")\n",
    "ax2.set_xlabel(\"User index\")\n",
    "ax2.set_ylabel(\"r_i\")\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"linreg_K{K}_FedOT_vs_FedAvg_full_with_priors.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(f\"linreg_K{K}_FedOT_vs_FedAvg_full_with_priors.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FedOT vs FedAvg(K) vs FedAvg(full) on MNIST (Generic K, Monte Carlo q from r)\n",
    "# Colors: FedOT=blue, FedAvg(K)=orange, FedAvg(full)=red\n",
    "# ================================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from itertools import combinations\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment Configuration\n",
    "# -----------------------------\n",
    "NUM_USERS = 100\n",
    "NUM_CLASSES = 10\n",
    "K = 2                          # generic K (size of each participating subset)\n",
    "ROUNDS = 2000                  # 2000 is fine too; 1000 for quicker runs\n",
    "LOCAL_EPOCHS = 3               # local steps per selection\n",
    "DIM = 784\n",
    "TOTAL_SAMPLES = 3000           # total examples used from MNIST\n",
    "SAMPLES_PER_USER = TOTAL_SAMPLES // NUM_USERS\n",
    "LR = 0.1\n",
    "SEEDS = [0, 1, 2, 3, 4]              # replicate for mean±std\n",
    "IPFP_TOL = 1e-10\n",
    "IPFP_MAX_ITERS = 4000\n",
    "NUM_SAMPLES_FOR_Q = 1_000_000  # MC draws to estimate q over K-subsets\n",
    "\n",
    "# -----------------------------\n",
    "# Fetch & preprocess MNIST once\n",
    "# -----------------------------\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X_full, y_full = mnist.data[:TOTAL_SAMPLES] / 255.0, mnist.target.astype(int)[:TOTAL_SAMPLES]\n",
    "X_full = StandardScaler().fit_transform(X_full)\n",
    "\n",
    "# ================================================================\n",
    "# Masked IPFP on an arbitrary K-subset family (1-based subset tuples)\n",
    "# ================================================================\n",
    "def build_mask(n, subsets):\n",
    "    m = len(subsets)\n",
    "    M = np.zeros((n, m), dtype=bool)\n",
    "    for j, Aj in enumerate(subsets):\n",
    "        for i in Aj:       # i in 1..n\n",
    "            M[i-1, j] = True\n",
    "    return M\n",
    "\n",
    "def initialize_Y(p, q, M, mode=\"uniform\"):\n",
    "    n, m = M.shape\n",
    "    Y = np.zeros((n, m), dtype=float)\n",
    "    for j in range(m):\n",
    "        rows = np.where(M[:, j])[0]\n",
    "        if len(rows) == 0:\n",
    "            if q[j] > 0:\n",
    "                raise ValueError(f\"Column j={j} has empty subset but q[j]={q[j]}>0.\")\n",
    "            else:\n",
    "                continue\n",
    "        if q[j] == 0.0:\n",
    "            continue\n",
    "        if mode == \"uniform\":\n",
    "            Y[rows, j] = q[j] / len(rows)\n",
    "        elif mode == \"p_proportional\":\n",
    "            pj = p[rows]\n",
    "            s = pj.sum()\n",
    "            if s > 0:\n",
    "                Y[rows, j] = q[j] * (pj / s)\n",
    "            else:\n",
    "                Y[rows, j] = q[j] / len(rows)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown init mode\")\n",
    "    return Y\n",
    "\n",
    "def ipfp_masked(p, q, M, tol=1e-10, max_iter=10000, verbose=False):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    n, m = M.shape\n",
    "    if p.ndim != 1 or q.ndim != 1:\n",
    "        raise ValueError(\"p and q must be 1D arrays\")\n",
    "    if not np.all(p >= 0) or not np.all(q >= 0):\n",
    "        raise ValueError(\"p and q must be nonnegative\")\n",
    "    if abs(p.sum() - q.sum()) > 1e-12:\n",
    "        raise ValueError(f\"Totals must match: sum(p)={p.sum()} vs sum(q)={q.sum()}\")\n",
    "\n",
    "    Y = initialize_Y(p, q, M, mode=\"uniform\")\n",
    "    rows_idx = [np.where(M[i, :])[0] for i in range(n)]\n",
    "    cols_idx = [np.where(M[:, j])[0] for j in range(m)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Row scaling\n",
    "        row_sums = Y.sum(axis=1)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            row_scale = np.ones(n)\n",
    "            need = (row_sums > 0)\n",
    "            row_scale[need] = p[need] / row_sums[need]\n",
    "        infeasible_rows = (row_sums == 0) & (p > 0)\n",
    "        if np.any(infeasible_rows):\n",
    "            i = np.where(infeasible_rows)[0][0]\n",
    "            raise ValueError(f\"Infeasible: row i={i} has no support but p[i]={p[i]}>0\")\n",
    "        for i in range(n):\n",
    "            js = rows_idx[i]\n",
    "            if js.size:\n",
    "                Y[i, js] *= row_scale[i]\n",
    "\n",
    "        # Column scaling\n",
    "        col_sums = Y.sum(axis=0)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            col_scale = np.ones(m)\n",
    "            needc = (col_sums > 0)\n",
    "            col_scale[needc] = q[needc] / col_sums[needc]\n",
    "        infeasible_cols = (col_sums == 0) & (q > 0)\n",
    "        if np.any(infeasible_cols):\n",
    "            j = np.where(infeasible_cols)[0][0]\n",
    "            raise ValueError(f\"Infeasible: column j={j} has no support but q[j]={q[j]}>0\")\n",
    "        for j in range(m):\n",
    "            is_ = cols_idx[j]\n",
    "            if is_.size:\n",
    "                Y[is_, j] *= col_scale[j]\n",
    "\n",
    "        row_err = np.linalg.norm(Y.sum(axis=1) - p, ord=1)\n",
    "        col_err = np.linalg.norm(Y.sum(axis=0) - q, ord=1)\n",
    "        if verbose and it % 200 == 0:\n",
    "            print(f\"[{it}] row_err={row_err:.3e}, col_err={col_err:.3e}\")\n",
    "        if max(row_err, col_err) <= tol:\n",
    "            return Y, {\"iters\": it+1, \"row_err\": row_err, \"col_err\": col_err}\n",
    "\n",
    "    return Y, {\"iters\": max_iter, \"row_err\": row_err, \"col_err\": col_err, \"warning\": \"max_iter reached\"}\n",
    "\n",
    "def recover_T_from_Y(Y, q, M, p, strategy_for_q0=\"p_restricted\"):\n",
    "    n, m = Y.shape\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    T = np.zeros_like(Y)\n",
    "\n",
    "    pos = q > 0\n",
    "    T[:, pos] = Y[:, pos] / q[pos]\n",
    "\n",
    "    zero_cols = np.where(~pos)[0]\n",
    "    for j in zero_cols:\n",
    "        rows = np.where(M[:, j])[0]\n",
    "        if rows.size == 0:\n",
    "            raise ValueError(f\"Column j={j} has empty subset; cannot define T.\")\n",
    "        if strategy_for_q0 == \"p_restricted\":\n",
    "            w = p[rows]\n",
    "            s = w.sum()\n",
    "            if s > 0:\n",
    "                T[rows, j] = w / s\n",
    "            else:\n",
    "                T[rows, j] = 1.0 / rows.size\n",
    "        elif strategy_for_q0 == \"uniform\":\n",
    "            T[rows, j] = 1.0 / rows.size\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy_for_q0\")\n",
    "\n",
    "    T[~M] = 0.0\n",
    "    col_sums = T.sum(axis=0)\n",
    "    for j in range(m):\n",
    "        if col_sums[j] != 0:\n",
    "            T[:, j] /= col_sums[j]\n",
    "    return T\n",
    "\n",
    "def solve_T_with_given_subsets(p, q, subsets, tol=1e-10, max_iter=10000, verbose=False):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    n = p.size\n",
    "    m = len(subsets)\n",
    "    if q.size != m:\n",
    "        raise ValueError(f\"Length of q ({q.size}) must equal number of subsets ({m}).\")\n",
    "\n",
    "    M = build_mask(n, subsets)  # 1-based\n",
    "    if abs(p.sum() - q.sum()) > 1e-12:\n",
    "        p = p / p.sum()\n",
    "        q = q / q.sum()\n",
    "\n",
    "    Y, info = ipfp_masked(p, q, M, tol=tol, max_iter=max_iter, verbose=verbose)\n",
    "    T = recover_T_from_Y(Y, q, M, p, strategy_for_q0=\"p_restricted\")\n",
    "\n",
    "    col_err = np.max(np.abs(T.sum(axis=0) - 1))\n",
    "    recon_p = T @ q\n",
    "    p_err = np.max(np.abs(recon_p - p))\n",
    "    info.update({\"T_col_err_inf\": float(col_err), \"p_match_err_inf\": float(p_err)})\n",
    "    return T, subsets, M, info\n",
    "\n",
    "def column_users_and_weights(T, subsets, j):\n",
    "    Aj_1 = subsets[j]                      # 1-based subset tuple\n",
    "    rows = np.array([i-1 for i in Aj_1])   # to 0-based indices\n",
    "    w = T[rows, j]\n",
    "    s = w.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        w = np.full_like(w, 1.0 / len(w))\n",
    "    else:\n",
    "        w = w / s\n",
    "    return rows, w\n",
    "\n",
    "# ================================================================\n",
    "# Distributions and subset family helpers\n",
    "# ================================================================\n",
    "def make_skew_distributions(N):\n",
    "    idx_1 = np.arange(1, N + 1, dtype=float)\n",
    "    # r_i ∝ exp(+i) (availability prior)\n",
    "    r = np.ones(N)\n",
    "    r /= r.sum()\n",
    "    # p_i ∝ exp(-i) (importance)\n",
    "    p = np.power(idx_1[::-1], 4)\n",
    "    p /= p.sum()\n",
    "    print(r)\n",
    "    print(p)\n",
    "    return p, r\n",
    "\n",
    "def all_K_subsets_1based(N, K):\n",
    "    return [tuple(c) for c in combinations(range(1, N + 1), K)]\n",
    "\n",
    "def estimate_q_by_mc(subsets, r, N, K, num_samples=1_000_000, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(0)\n",
    "    subset_to_idx = {s: j for j, s in enumerate(subsets)}\n",
    "    draws = rng.choice(N, size=(num_samples, K), replace=True, p=r)\n",
    "    draws.sort(axis=1)\n",
    "    draws_1b = draws + 1\n",
    "    uniq, counts = np.unique(draws_1b, axis=0, return_counts=True)\n",
    "    q_counts = np.zeros(len(subsets), dtype=np.int64)\n",
    "    for row, c in zip(uniq, counts):\n",
    "        tup = tuple(row.tolist())\n",
    "        j = subset_to_idx.get(tup, None)\n",
    "        if j is not None:\n",
    "            q_counts[j] += c\n",
    "    q = q_counts.astype(float)\n",
    "    q /= q.sum()\n",
    "    return q\n",
    "\n",
    "# ================================================================\n",
    "# Multinomial logistic regression (NumPy)\n",
    "# ================================================================\n",
    "def init_theta(d, C):\n",
    "    return {\"W\": np.zeros((d, C)), \"b\": np.zeros(C)}\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def forward(X, theta):\n",
    "    return X @ theta[\"W\"] + theta[\"b\"]\n",
    "\n",
    "def ce_loss_and_grad(X, y, theta, l2=0.0):\n",
    "    n, d = X.shape\n",
    "    C = theta[\"W\"].shape[1]\n",
    "    scores = forward(X, theta)\n",
    "    P = softmax(scores)\n",
    "    Y = np.zeros((n, C))\n",
    "    Y[np.arange(n), y] = 1.0\n",
    "    eps = 1e-12\n",
    "    loss = -np.sum(Y * np.log(P + eps)) / n\n",
    "    loss += 0.5 * l2 * np.sum(theta[\"W\"] ** 2)\n",
    "    G = (P - Y) / n\n",
    "    dW = X.T @ G + l2 * theta[\"W\"]\n",
    "    db = np.sum(G, axis=0)\n",
    "    return loss, {\"W\": dW, \"b\": db}\n",
    "\n",
    "def local_train(theta, X, y, epochs=1, lr=0.1, l2=0.0):\n",
    "    th = {\"W\": theta[\"W\"].copy(), \"b\": theta[\"b\"].copy()}\n",
    "    for _ in range(epochs):\n",
    "        _, grads = ce_loss_and_grad(X, y, th, l2=l2)\n",
    "        th[\"W\"] -= lr * grads[\"W\"]\n",
    "        th[\"b\"] -= lr * grads[\"b\"]\n",
    "    return th\n",
    "\n",
    "def weighted_average_thetas(thetas, weights):\n",
    "    W = np.sum([w * t[\"W\"] for t, w in zip(thetas, weights)], axis=0)\n",
    "    b = np.sum([w * t[\"b\"] for t, w in zip(thetas, weights)], axis=0)\n",
    "    return {\"W\": W, \"b\": b}\n",
    "\n",
    "def global_loss(theta, user_data, p, l2=0.0):\n",
    "    tot = 0.0\n",
    "    for i, (Xi, yi) in enumerate(user_data):\n",
    "        li, _ = ce_loss_and_grad(Xi, yi, theta, l2=l2)\n",
    "        tot += p[i] * li\n",
    "    return tot\n",
    "\n",
    "# ================================================================\n",
    "# Data partitioning\n",
    "# ================================================================\n",
    "def make_user_data(X_full, y_full, N):\n",
    "    perm = np.random.permutation(len(X_full))\n",
    "    X = X_full[perm]\n",
    "    y = y_full[perm]\n",
    "    return [(X[i*SAMPLES_PER_USER:(i+1)*SAMPLES_PER_USER],\n",
    "             y[i*SAMPLES_PER_USER:(i+1)*SAMPLES_PER_USER]) for i in range(N)]\n",
    "\n",
    "# ================================================================\n",
    "# Run the three methods per seed\n",
    "# ================================================================\n",
    "all_losses_fedot = []\n",
    "all_losses_faK   = []\n",
    "all_losses_full  = []\n",
    "print(f\"\\n[Seed {seed}]  (K={K})\")\n",
    "rng = np.random.RandomState(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# user datasets\n",
    "user_data = make_user_data(X_full, y_full, NUM_USERS)\n",
    "\n",
    "# skewed p and r\n",
    "p, r = make_skew_distributions(NUM_USERS)\n",
    "\n",
    "# K-subsets and q via Monte Carlo from r\n",
    "subsets_K = all_K_subsets_1based(NUM_USERS, K)\n",
    "print(f\"  Total subsets size K: C({NUM_USERS},{K}) = {len(subsets_K)}\")\n",
    "q = estimate_q_by_mc(subsets_K, r, NUM_USERS, K, num_samples=NUM_SAMPLES_FOR_Q, rng=rng)\n",
    "\n",
    "# Solve T for this family\n",
    "T, subsets_used, M_mask, info = solve_T_with_given_subsets(\n",
    "    p, q, subsets_K, tol=IPFP_TOL, max_iter=IPFP_MAX_ITERS, verbose=False\n",
    ")\n",
    "print(f\"  IPFP iters={info.get('iters')} | T col err∞={info.get('T_col_err_inf'):.2e} | p match err∞={info.get('p_match_err_inf'):.2e}\")\n",
    "\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n[Seed {seed}]  (K={K})\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # initialize global models\n",
    "    theta_fedot = init_theta(DIM, NUM_CLASSES)\n",
    "    theta_faK   = init_theta(DIM, NUM_CLASSES)\n",
    "    theta_full  = init_theta(DIM, NUM_CLASSES)\n",
    "\n",
    "    losses_ot, losses_faK, losses_full = [], [], []\n",
    "\n",
    "    # Precompute for speed\n",
    "    Xs = [ud[0] for ud in user_data]\n",
    "    ys = [ud[1] for ud in user_data]\n",
    "    q_cum = np.cumsum(q)\n",
    "\n",
    "    for t in range(ROUNDS):\n",
    "        # sample subset j ~ q\n",
    "        u = rng.random()\n",
    "        j = int(np.searchsorted(q_cum, u, side=\"right\"))\n",
    "        users_S, weights_T = column_users_and_weights(T, subsets_used, j)  # indices (0-based) and normalized weights\n",
    "\n",
    "        # ------------------ FedOT(K) ------------------\n",
    "        local_thetas = []\n",
    "        for uid in users_S:\n",
    "            ti = local_train(theta_fedot, Xs[uid], ys[uid], epochs=LOCAL_EPOCHS, lr=LR, l2=0.0)\n",
    "            local_thetas.append(ti)\n",
    "        theta_fedot = weighted_average_thetas(local_thetas, weights_T)\n",
    "        losses_ot.append(global_loss(theta_fedot, user_data, p, l2=0.0))\n",
    "\n",
    "        # ---------------- FedAvg(K) EXACT: sum_{i in S} (N/K)*p_i * theta_i ----------------\n",
    "        local_thetas_fak = []\n",
    "        for uid in users_S:\n",
    "            ti = local_train(theta_faK, Xs[uid], ys[uid], epochs=LOCAL_EPOCHS, lr=LR, l2=0.0)\n",
    "            local_thetas_fak.append((uid, ti))\n",
    "        scale = NUM_USERS / float(K)\n",
    "        # weighted sum without normalization (EXACT rule)\n",
    "        W_acc = np.zeros_like(theta_faK[\"W\"])\n",
    "        b_acc = np.zeros_like(theta_faK[\"b\"])\n",
    "        for uid, ti in local_thetas_fak:\n",
    "            coeff = p[uid] * scale\n",
    "            W_acc += coeff * ti[\"W\"]\n",
    "            b_acc += coeff * ti[\"b\"]\n",
    "        theta_faK = {\"W\": W_acc, \"b\": b_acc}\n",
    "        losses_faK.append(global_loss(theta_faK, user_data, p, l2=0.0))\n",
    "\n",
    "        # ---------------- FedAvg (full): sum_i p_i * theta_i ----------------\n",
    "        W_acc = np.zeros_like(theta_full[\"W\"])\n",
    "        b_acc = np.zeros_like(theta_full[\"b\"])\n",
    "        for uid in range(NUM_USERS):\n",
    "            ti = local_train(theta_full, Xs[uid], ys[uid], epochs=LOCAL_EPOCHS, lr=LR, l2=0.0)\n",
    "            W_acc += p[uid] * ti[\"W\"]\n",
    "            b_acc += p[uid] * ti[\"b\"]\n",
    "        theta_full = {\"W\": W_acc, \"b\": b_acc}\n",
    "        losses_full.append(global_loss(theta_full, user_data, p, l2=0.0))\n",
    "\n",
    "        if (t + 1) % 200 == 0:\n",
    "            print(f\"  round {t+1}/{ROUNDS}  FedOT={losses_ot[-1]:.4f} | \"\n",
    "                  f\"FedAvg(K)={losses_faK[-1]:.4f} | FedAvg(full)={losses_full[-1]:.4f}\")\n",
    "\n",
    "    all_losses_fedot.append(np.array(losses_ot))\n",
    "    all_losses_faK.append(np.array(losses_faK))\n",
    "    all_losses_full.append(np.array(losses_full))\n",
    "\n",
    "# ================================================================\n",
    "# Aggregate and plot (loss curves + p/r bars)\n",
    "# ================================================================\n",
    "L_ot   = np.vstack(all_losses_fedot)\n",
    "L_faK  = np.vstack(all_losses_faK)\n",
    "L_full = np.vstack(all_losses_full)\n",
    "\n",
    "mean_ot,  std_ot  = L_ot.mean(axis=0),  L_ot.std(axis=0)\n",
    "mean_faK, std_faK = L_faK.mean(axis=0), L_faK.std(axis=0)\n",
    "mean_full,std_full= L_full.mean(axis=0), L_full.std(axis=0)\n",
    "\n",
    "# p and r (deterministic functions of N)\n",
    "p, r = make_skew_distributions(NUM_USERS)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[2.2, 1.0])\n",
    "\n",
    "# --- Top row: loss curves\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "x = np.arange(ROUNDS)\n",
    "\n",
    "# FedOT (blue)\n",
    "ax0.plot(x, mean_ot, label=f\"FedOT (K={K})\", color=\"tab:blue\", linewidth=1.8)\n",
    "ax0.fill_between(x, mean_ot - std_ot, mean_ot + std_ot, alpha=0.15, color=\"tab:blue\")\n",
    "\n",
    "# FedAvg(K) (orange)\n",
    "ax0.plot(x, mean_faK, label=f\"FedAvg (K={K})\", color=\"tab:orange\", linewidth=1.8)\n",
    "ax0.fill_between(x, mean_faK - std_faK, mean_faK + std_faK, alpha=0.15, color=\"tab:orange\")\n",
    "\n",
    "# FedAvg (full) (red)\n",
    "ax0.plot(x, mean_full, label=\"FedAvg (full devices)\", color=\"tab:red\", linewidth=1.8)\n",
    "ax0.fill_between(x, mean_full - std_full, mean_full + std_full, alpha=0.15, color=\"tab:red\")\n",
    "\n",
    "# ax0.set_yscale(\"log\")  # optional\n",
    "ax0.set_xlabel(\"Communication round\")\n",
    "ax0.set_ylabel(r\"Global loss $\\sum_i p_i f_i(\\theta)$\")\n",
    "ax0.set_title(f\"MNIST — FedOT vs FedAvg (K={K}) vs FedAvg(full) — mean ± std over {len(SEEDS)} seeds\")\n",
    "ax0.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\")\n",
    "ax0.legend()\n",
    "\n",
    "# --- Bottom left: p bar plot\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "ax1.bar(np.arange(1, NUM_USERS+1), p, color=\"gray\", edgecolor=\"black\", linewidth=0.6)\n",
    "ax1.set_title(\"Posterior Importance Distribution $p$\")\n",
    "ax1.set_xlabel(\"User index\")\n",
    "ax1.set_ylabel(\"p_i\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# --- Bottom right: r bar plot\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "ax2.bar(np.arange(1, NUM_USERS+1), r, color=\"gray\", edgecolor=\"black\", linewidth=0.6)\n",
    "ax2.set_title(\"Prior User Selection Probability Marginals\")\n",
    "ax2.set_xlabel(\"User index\")\n",
    "ax2.set_ylabel(\"r_i\")\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"mnist_K{K}_FedOT_vs_FedAvg_full_with_priors.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(f\"mnist_K{K}_FedOT_vs_FedAvg_full_with_priors.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
